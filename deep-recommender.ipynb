{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDREc3aV7W8G",
    "outputId": "11c5c04c-a348-4830-ec8e-79b1599a88ac"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "import os\n",
    "from os import path, getcwd, listdir, chdir\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# from google.colab import drive \n",
    "# drive.mount('/content/drive')\n",
    "# chdir('/content/drive/My Drive/Colab Notebooks/Data_Imputation/')\n",
    "\n",
    "SEED = 121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uyzudC7T7cxy"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/train_set_v3.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.read_csv('./data/test_set_without_yield_v3.txt', sep='\\t')\n",
    "# submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.groupby([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "1KQFJ_WY7oxT",
    "outputId": "f2783115-1d59-4a63-94cd-bfc64fb9b31f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hybrid</th>\n",
       "      <th>Experiment code</th>\n",
       "      <th>Year</th>\n",
       "      <th>Location</th>\n",
       "      <th>Yield</th>\n",
       "      <th>Inbred 1</th>\n",
       "      <th>Inbred 2</th>\n",
       "      <th>Inbred 1 genetic group</th>\n",
       "      <th>Inbred 2 genetic group</th>\n",
       "      <th>trial_cluster_level1</th>\n",
       "      <th>trial_cluster_level2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hybrid_0</td>\n",
       "      <td>trial_0</td>\n",
       "      <td>2014</td>\n",
       "      <td>loc_0</td>\n",
       "      <td>240.28</td>\n",
       "      <td>parent_0</td>\n",
       "      <td>parent_2215</td>\n",
       "      <td>genetic_group_0</td>\n",
       "      <td>genetic_group_33</td>\n",
       "      <td>03</td>\n",
       "      <td>03b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Hybrid Experiment code  Year Location   Yield  Inbred 1     Inbred 2  \\\n",
       "0  hybrid_0         trial_0  2014    loc_0  240.28  parent_0  parent_2215   \n",
       "\n",
       "  Inbred 1 genetic group  Inbred 2 genetic group trial_cluster_level1  \\\n",
       "0         genetic_group_0       genetic_group_33                   03   \n",
       "\n",
       "  trial_cluster_level2  \n",
       "0                  03b  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3GDVTBnA9PLZ",
    "outputId": "c9e7a331-d568-4e5d-c48c-213a98255ddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523367 train examples\n",
      "10749 validation examples\n",
      "8796 test examples\n"
     ]
    }
   ],
   "source": [
    "data = data.drop(columns=['Hybrid'])\n",
    "data.fillna('UNK', inplace=True) \n",
    "data = data.sample(len(data), replace=False, random_state=SEED)\n",
    "train, test = train_test_split(data, test_size=0.036, random_state=SEED)\n",
    "val, test = train_test_split(test, test_size=0.45, random_state=SEED)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')\n",
    "\n",
    "# create scaler\n",
    "# scaler = MinMaxScaler()\n",
    "# # fit scaler on data\n",
    "# scaler.fit(np.reshape(train['Yield'].values, (-1, 1)))\n",
    "# # Fit new values\n",
    "# train['Yield'] = scaler.transform(np.reshape(train['Yield'].values, (-1, 1))).squeeze()\n",
    "# val['Yield'] = scaler.transform(np.reshape(val['Yield'].values, (-1, 1))).squeeze()\n",
    "# test['Yield'] = scaler.transform(np.reshape(test['Yield'].values, (-1, 1))).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_preparation(train, test):\n",
    "#     results = dict()\n",
    "#     test = test.copy()\n",
    "    \n",
    "#     cols = train.columns\n",
    "#     loc_grp = train.groupby(['Location'])['Yield'].agg(\n",
    "#         yield_mean='mean', std_yied='std').reset_index()\n",
    "#     results['operations'] = {}\n",
    "#     results['operations'][0] = loc_grp\n",
    "#     train_loc = pd.merge(train, loc_grp, on='Location')\n",
    "#     train_loc['Yield'] = (train_loc['Yield'] - train_loc['yield_mean']) #/(train_loc['std_yied'])\n",
    "#     train_loc = train_loc[cols]\n",
    "#     # test\n",
    "#     test = pd.merge(test, loc_grp, on='Location')\n",
    "#     test['Yield'] = (test['Yield'] - test['yield_mean']) #/ test['std_yied']\n",
    "#     test = test[cols]\n",
    "    \n",
    "#     # second operation\n",
    "#     cluster1_grp = train_loc.groupby(['trial_cluster_level1'])['Yield'].agg(yield_mean='mean', std_yied='std').reset_index()\n",
    "#     train_cluster = pd.merge(train_loc, cluster1_grp, on='trial_cluster_level1')\n",
    "#     train_cluster['Yield'] = (train_cluster['Yield'] - train_cluster['yield_mean']) #/ (train_cluster['std_yied'])\n",
    "#     results['operations'][1] = cluster1_grp\n",
    "#     train_cluster = train_cluster[cols]\n",
    "#     # test\n",
    "#     test = pd.merge(test, cluster1_grp, on='trial_cluster_level1')\n",
    "#     test['Yield'] = (test['Yield'] - test['yield_mean']) #/ test['std_yied']\n",
    "#     test = test[cols]\n",
    "    \n",
    "#     results['train'] = train_cluster\n",
    "#     results['test'] = test\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = data_preparation(train_, val_)\n",
    "# train = results['train']\n",
    "# val = results['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "u_Xbdao0-T9I"
   },
   "outputs": [],
   "source": [
    "# labels = dataframe.pop('Yield')\n",
    "\n",
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32, labels=None, feature_cols=None):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('Yield')\n",
    "#   features_cols = ['hybrid', 'trial', 'year', 'location', 'inbred1', 'inbred2', 'genetic_inb1', 'genetic_inb2', 'trial_cl1', 'trial_cl2']\n",
    "  dataframe.columns = features_cols #['hybrid', 'trial', 'year', 'location', 'inbred1', 'inbred2', 'genetic_inb1', 'genetic_inb2', 'trial_cl1', 'trial_cl2']\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zrZJ7iBK_Np4"
   },
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for our feature.\n",
    "  normalizer = preprocessing.Normalization()\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCrD6Z4sNSEX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jK6Rues9_k-c"
   },
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, vocab=None, max_tokens=None):\n",
    "  # Create a StringLookup layer which will turn strings into integer indices\n",
    "  vocab = np.unique(np.concatenate(list(dataset.map(lambda x, _: x[name]))))\n",
    "  if dtype == 'string':\n",
    "    index = preprocessing.StringLookup(vocabulary=vocab)\n",
    "  else:\n",
    "    index = preprocessing.IntegerLookup(vocabulary=vocab)\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "#   index.adapt(feature_ds)\n",
    "\n",
    "  # Create a Discretization for our integer indices.\n",
    "  encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = feature_ds.map(index)\n",
    "\n",
    "  # Learn the space of possible indices.\n",
    "  encoder.adapt(feature_ds)\n",
    "\n",
    "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "  # layer so we can use them, or include them in the functional model later.\n",
    "  return lambda feature: encoder(index(feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sxw-wEURBdTp"
   },
   "outputs": [],
   "source": [
    "features_cols = ['trial', 'year', 'location', \n",
    "                 'inbred1', 'inbred2', \n",
    "                 'genetic_inb1', \n",
    "                 'genetic_inb2', 'trial_cl1', 'trial_cl2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Lvl_XhJpBHrq"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size, feature_cols=features_cols)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size, feature_cols=features_cols)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size, feature_cols=features_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AXef84ah_tES"
   },
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = {}\n",
    "\n",
    "# Numeric features.\n",
    "for header in []: # nothing here for now\n",
    "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "  normalization_layer = get_normalization_layer(header, train_ds)\n",
    "  encoded_numeric_col = normalization_layer(numeric_col)\n",
    "  all_inputs.append(numeric_col)\n",
    "  encoded_features[header] = encoded_numeric_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_normalization_encoding(header, train_ds):\n",
    "    numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "    normalization_layer = get_normalization_layer(header, train_ds)\n",
    "    encoded_numeric_col = normalization_layer(numeric_col)\n",
    "    return numeric_col, encoded_numeric_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2pZ52U6uAifm"
   },
   "outputs": [],
   "source": [
    "# Categorical features encoded as integers.\n",
    "categorical_nums = ['year']\n",
    "for header in categorical_nums:\n",
    "  year_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "  encoding_layer = get_category_encoding_layer(header, train_ds, dtype='int64')\n",
    "  encoded_year_col = encoding_layer(year_col)\n",
    "  all_inputs.append(year_col)\n",
    "  encoded_features[header] = encoded_year_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_categorical_encoding(header, train_ds):\n",
    "    int_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "    encoding_layer = get_category_encoding_layer(header, train_ds, dtype='int64')\n",
    "    encoded_int_col = encoding_layer(int_col)\n",
    "    return int_col, encoded_int_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ixRrba7YCjTT"
   },
   "outputs": [],
   "source": [
    "# Categorical features encoded as string.\n",
    "# categorical_cols = [name  for name in train.columns if train[name].dtype not in ['int64', 'float64', 'float32']]\n",
    "categorical_cols = ['trial','location', \n",
    "                    'inbred1', 'inbred2', \n",
    "                    'genetic_inb1', 'genetic_inb2', 'trial_cl1', 'trial_cl2']\n",
    "for header in categorical_cols:\n",
    "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "  encoding_layer = get_category_encoding_layer(header, train_ds, dtype='string')\n",
    "  encoded_categorical_col = encoding_layer(categorical_col)\n",
    "  all_inputs.append(categorical_col)\n",
    "  encoded_features[header] = encoded_categorical_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_encoding(header, train_ds):\n",
    "    categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "    encoding_layer = get_category_encoding_layer(header, train_ds, dtype='string')\n",
    "    encoded_categorical_col = encoding_layer(categorical_col)\n",
    "    return categorical_col, encoded_categorical_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_col(header, train_ds, dtype=None):\n",
    "    if dtype is None:\n",
    "        for train, y in train_ds.as_numpy_iterator():\n",
    "            dtype = train[header].dtype\n",
    "            break\n",
    "#         break\n",
    "    dtypes = {\n",
    "        object: 'string',\n",
    "        float: float,\n",
    "        'int64': 'int64'\n",
    "    }\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "sr7HOxtrXzVF"
   },
   "outputs": [],
   "source": [
    "data_features = data[[ 'Experiment code', 'Year', 'Location', \n",
    "                      'Inbred 1', 'Inbred 2', \n",
    "                      'Inbred 1 genetic group ', 'Inbred 2 genetic group',\n",
    "                      'trial_cluster_level1', 'trial_cluster_level2']]\n",
    "data_features.columns = features_cols\n",
    "\n",
    "embeddings_voc_length_dict = {\n",
    "    col: len(data_features[col].unique()) + 1 for col in data_features.columns\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "b9TR9B_4PPSH"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "\n",
    "embeddings = {\n",
    "    name + '_embedding': keras.layers.Embedding(\n",
    "        name=name+'_emb',\n",
    "        input_dim=length,\n",
    "        output_dim=embedding_dim)(encoded_features[name])\n",
    "    for name, length in embeddings_voc_length_dict.items()\n",
    "}\n",
    "\n",
    "wide_embeddings = {\n",
    "    name + '_embedding': keras.layers.Embedding(\n",
    "        name=name+'_emb',\n",
    "        input_dim=length,\n",
    "        output_dim=embedding_dim)(encoded_features[name])\n",
    "    for name, length in embeddings_voc_length_dict.items() if name.startswith('inbred')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings(embedding_dim, encoded_features):\n",
    "    embeddings = {\n",
    "        name + '_embedding': keras.layers.Embedding(\n",
    "            name=name+'_emb',\n",
    "            input_dim=length,\n",
    "        output_dim=embedding_dim)(encoded_features[name])\n",
    "        for name, length in embeddings_voc_length_dict.items()\n",
    "    }\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "pIO_27M0lPY9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "912"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNModel(tfrs.Model):\n",
    "    def __init__(self, embeddings, all_inputs, num_cross_layers, deep_layers, projection_dim=None, **kwargs):\n",
    "        self._data_checker(embeddings, num_cross_layers, deep_layers)\n",
    "        super(DCNModel, self).__init__(**kwargs)\n",
    "        self.projection_dim = projection_dim\n",
    "        self.embeddings = embeddings\n",
    "#         self.all_inputs = all_inputs\n",
    "        self.num_cross_layers = num_cross_layers\n",
    "        self.deep_layers = deep_layers\n",
    "        \n",
    "#         self.combined_embeddings = tf.keras.layers.concatenate(list(self.embeddings.values()), axis=1)\n",
    "        self._crosses = [tfrs.layers.dcn.Cross(projection_dim=self.projection_dim, name=f'cross_{i}') \n",
    "                        for i in range(self.num_cross_layers)]\n",
    "        self._flatten = tf.keras.layers.Flatten()\n",
    "        self._denses = [tf.keras.layers.Dense(units, activation='relu', name=f'dense_{i}') \n",
    "                       for i,units in enumerate(self.deep_layers)] \n",
    "        self._batch_norm = tf.keras.layers.BatchNormalization()\n",
    "        self._output = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def _data_checker(self, embeddings, all_inputs, num_cross_layers, deep_layers):\n",
    "        assert isinstance(embeddings, dict), f'{embeddings} should be a \"dict\" but you provided a {type(embeddings)}'\n",
    "        assert isinstance(num_cross_layers, int) or isinstance(num_cross_layers, float), f'{num_cross_layers} should be int or float'\n",
    "        assert isinstance(deep_layers, list), f'{deep_layers} should be a \"list\" but you provided a {type(deep_layers)}'\n",
    "#         assert isinstance(all_inputs, list), f'{all_inputs} should be a list of tensors'\n",
    "        \n",
    "    def call(self, all_inputs):\n",
    "        combined_embeddings = tf.keras.layers.concatenate(list(self.embeddings.values()), axis=1)(all_inputs)\n",
    "        for i, cross in enumerate(self._crosses):\n",
    "            if i == 0:\n",
    "                y = cross(combined_embeddings)\n",
    "            else:\n",
    "                y = cross(combined_embeddings, y)\n",
    "        # add more layers later\n",
    "        x = self._flatten(y)\n",
    "        for cnter, layer in enumerate(self._denses):\n",
    "            x = layer(x)\n",
    "            if cnter == 0:\n",
    "                x = self._batch_norm(x)\n",
    "        x = self._output(x)\n",
    "#         model = tf.keras.models.Model(inputs=all_inputs, outputs=x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRNetBlock(tf.keras.Model):\n",
    "    '''Creates Individual Residual Network'''\n",
    "    def __init__(self, num_reurons, activation='relu', **kwargs):\n",
    "        super(IRNetBlock, self).__init__(**kwargs)\n",
    "        self.num_neurons = num_neurons\n",
    "        self._dense = tf.keras.layers.Dense(self.num_neurons)\n",
    "        self._act = tf.keras.layers.Activation(activation)\n",
    "        self._bn = tf.keras.layers.BatchNormalization()\n",
    "        self._add = tf.keras.layers.Add()\n",
    "        self._dense_adjust = tf.keras.layers.Dense(num_neurons)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self._dense(inputs)\n",
    "#         x = self._bn(x)\n",
    "        y = inputs\n",
    "        if inputs.shape[1] != x.shape[1]:\n",
    "            y = self._dense_adjust(inputs)\n",
    "        x = self._add([y, x])\n",
    "        x = self._act(x)\n",
    "        return x\n",
    "    \n",
    "class SRNetBlock(tf.keras.Model):\n",
    "    ''' Creates a Stratified Residual Network'''\n",
    "    def __init__(self, num_neurons, num_layers, activation='relu', **kwargs):\n",
    "        super(SRNetBlock, self).__init__(**kwargs)\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_layers = num_layers\n",
    "        self._dense = [tf.keras.layers.Dense(num_neurons) for _ in range(num_layers)]\n",
    "        self._act = tf.keras.layers.Activation(activation)\n",
    "        self._bn = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
    "        self._add = tf.keras.layers.Add()\n",
    "        self._dense_adjust = tf.keras.layers.Dense(num_neurons)\n",
    "        self._bn_adjust = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = None\n",
    "        \n",
    "        for i, (dense, bn) in enumerate(zip(self._dense, self._bn)):\n",
    "            if i == 0:\n",
    "                x = dense(inputs)\n",
    "            else:\n",
    "                x = dense(x)\n",
    "#             x = bn(x)\n",
    "            # add the activation only for layers before the last one\n",
    "            if i < self.num_layers - 1:\n",
    "                x = self._act(x)\n",
    "        \n",
    "        y = inputs\n",
    "        if inputs.shape[1] != x.shape[1]:\n",
    "            logger.debug(f'x shape:{x.shape} and inputs shape: {inputs.shape}')\n",
    "            y = self._dense_adjust(inputs)\n",
    "#             y = self._bn_adjust(y)\n",
    "        x = self._add([y, x])\n",
    "        x = self._act(x)\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.keras.layers.Dense(128, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.compute_output_shape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "e9QcGGtEYpXJ"
   },
   "outputs": [],
   "source": [
    "# gene_env_year = tf.keras.layers.concatenate(list(embeddings.values()), axis=1)\n",
    "# wide_layer = tf.keras.layers.concatenate(list(wide_embeddings.values()), axis=1)\n",
    "\n",
    "# y = tfrs.layers.dcn.Cross(projection_dim=embedding_dim//2)(gene_env_year)\n",
    "# # y = tfrs.layers.dcn.Cross(projection_dim=embedding_dim//2)(y, gene_env_year)\n",
    "# # y = tfrs.layers.dcn.Cross(projection_dim=embedding_dim//2)(gene_env_year, y)\n",
    "\n",
    "# x = keras.layers.Flatten()(y)\n",
    "# x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "# # x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "# x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "# x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "# # x = tf.keras.layers.Dropout(0.1)(x)\n",
    "# # x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "# # x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "# # xy = tf.keras.layers.Concatenate()([x, y])\n",
    "# x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "# output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "# def define_model(num_neurons, num_layers, cross_network=True):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile(*,\n",
    "        optimizer, \n",
    "        loss,\n",
    "        metrics,\n",
    "        all_inputs,\n",
    "        deep_layers=None,\n",
    "        embedding_dim=None, \n",
    "        projection_dim=None,\n",
    "        encoded_features=None,\n",
    "        embeddings_voc_length_dict=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # get all inputs\n",
    "    \n",
    "#     all_inputs = get_all_inputs()\n",
    "    \n",
    "    embeddings = build_embeddings(embedding_dim, encoded_features)\n",
    "    \n",
    "    gene_env_year = tf.keras.layers.concatenate(list(embeddings.values()), axis=1)\n",
    "#     wide_layer = tf.keras.layers.concatenate(list(wide_embeddings.values()), axis=1)\n",
    "\n",
    "    y = tfrs.layers.dcn.Cross(projection_dim=embedding_dim//2)(gene_env_year)\n",
    "    x = keras.layers.Flatten()(y)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "#     x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
    "#     x = tf.keras.layers.Concatenate()([x, y])\n",
    "#     x = keras.layers.Flatten()(x)\n",
    "#     x = tf.keras.layers.Dense(128)(x)\n",
    "#     x = tf.keras.layers.BatchNormalization()(x)\n",
    "#     x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = SRNetBlock(128, 3)(x)\n",
    "    x = SRNetBlock(128, 3)(x)\n",
    "    x = SRNetBlock(64, 3)(x)\n",
    "#     x = SRNetBlock(64, 3)(x)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    model = keras.Model(all_inputs, output)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(tf.keras.losses.Loss):\n",
    "    \"\"\" Quantile loss \"\"\"\n",
    "#     learning_rate = .001\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "#         self.learning_rate = learning_rate\n",
    "    def call(self, pred, actual):\n",
    "        error = K.abs(pred - actual)\n",
    "        cond100 = error >= 5\n",
    "        cond50 = tf.math.logical_and(2.5 <= error, error < 5)\n",
    "        cond0 = error < 2.5\n",
    "        loss_100 = 100 * tf.square(error)\n",
    "        loss_50 = 50 * tf.square(error)\n",
    "        loss_0 = tf.square(error)\n",
    "\n",
    "        func_error = tf.where(cond100, loss_100, tf.where(cond50, loss_50, loss_0))\n",
    "        return K.mean(func_error, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam(learning_rate=5.0e-4, decay=1e-5)\n",
    "loss=tf.keras.losses.MeanSquaredError()\n",
    "metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mae']\n",
    "\n",
    "model = build_and_compile(\n",
    "        optimizer=optimizer, \n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "        all_inputs=all_inputs,\n",
    "        deep_layers=None,\n",
    "        embedding_dim=20, \n",
    "        projection_dim=20,\n",
    "        encoded_features=encoded_features,\n",
    "        embeddings_voc_length_dict=embeddings_voc_length_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "M8i_YSXGGNm2"
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.Model(all_inputs, output)\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1.0e-3, decay=1e-5),\n",
    "#               loss=tf.keras.losses.MeanSquaredError(),\n",
    "#               metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "CIgUAg1BGpTt",
    "outputId": "be299aa7-bfae-4d75-b9fe-c5628ea68e37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "trial (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "year (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "location (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inbred1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inbred2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "genetic_inb1 (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "genetic_inb2 (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "trial_cl1 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "trial_cl2 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup (StringLookup)    (None, 1)            0           trial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "integer_lookup (IntegerLookup)  (None, 1)            0           year[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_1 (StringLookup)  (None, 1)            0           location[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_2 (StringLookup)  (None, 1)            0           inbred1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_3 (StringLookup)  (None, 1)            0           inbred2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_4 (StringLookup)  (None, 1)            0           genetic_inb1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_5 (StringLookup)  (None, 1)            0           genetic_inb2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_6 (StringLookup)  (None, 1)            0           trial_cl1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_7 (StringLookup)  (None, 1)            0           trial_cl2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_1 (CategoryEn (None, 48894)        0           string_lookup[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding (CategoryEnco (None, 7)            0           integer_lookup[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_2 (CategoryEn (None, 376)          0           string_lookup_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_3 (CategoryEn (None, 2217)         0           string_lookup_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_4 (CategoryEn (None, 2375)         0           string_lookup_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_5 (CategoryEn (None, 35)           0           string_lookup_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_6 (CategoryEn (None, 36)           0           string_lookup_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_7 (CategoryEn (None, 19)           0           string_lookup_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_8 (CategoryEn (None, 34)           0           string_lookup_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "trial_emb (Embedding)           (None, 48894, 20)    978220      category_encoding_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "year_emb (Embedding)            (None, 7, 20)        120         category_encoding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "location_emb (Embedding)        (None, 376, 20)      7500        category_encoding_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inbred1_emb (Embedding)         (None, 2217, 20)     44320       category_encoding_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inbred2_emb (Embedding)         (None, 2375, 20)     47480       category_encoding_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "genetic_inb1_emb (Embedding)    (None, 35, 20)       680         category_encoding_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "genetic_inb2_emb (Embedding)    (None, 36, 20)       700         category_encoding_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "trial_cl1_emb (Embedding)       (None, 19, 20)       360         category_encoding_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "trial_cl2_emb (Embedding)       (None, 34, 20)       660         category_encoding_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 53993, 20)    0           trial_emb[0][0]                  \n",
      "                                                                 year_emb[0][0]                   \n",
      "                                                                 location_emb[0][0]               \n",
      "                                                                 inbred1_emb[0][0]                \n",
      "                                                                 inbred2_emb[0][0]                \n",
      "                                                                 genetic_inb1_emb[0][0]           \n",
      "                                                                 genetic_inb2_emb[0][0]           \n",
      "                                                                 trial_cl1_emb[0][0]              \n",
      "                                                                 trial_cl2_emb[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cross (Cross)                   (None, 53993, 20)    420         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1079860)      0           cross[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          138222208   flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sr_net_block (SRNetBlock)       (None, 128)          49536       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sr_net_block_1 (SRNetBlock)     (None, 128)          49536       sr_net_block[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sr_net_block_2 (SRNetBlock)     (None, 64)           24832       sr_net_block_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1)            65          sr_net_block_2[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 139,426,637\n",
      "Trainable params: 139,426,637\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# rankdir='LR' is used to make the graph horizontal.\n",
    "# tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "_SIGswAntE24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1046"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Nf4lo-nbCUKd"
   },
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_rmse', \n",
    "    verbose=.01,\n",
    "    patience=5,\n",
    "    mode='min', \n",
    "    min_delta=.1,\n",
    "    restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQG-eU4bGtNY",
    "outputId": "fc048f42-1b03-4df3-f766-c18a56b907de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "65421/65421 [==============================] - 8464s 129ms/step - loss: 606.5835 - rmse: 24.3330 - mae: 18.6220 - val_loss: 392.8072 - val_rmse: 19.8194 - val_mae: 14.9884\n",
      "Epoch 2/5\n",
      "65421/65421 [==============================] - 8602s 131ms/step - loss: 346.3818 - rmse: 18.6112 - mae: 14.0888 - val_loss: 375.2309 - val_rmse: 19.3709 - val_mae: 14.6122\n",
      "Epoch 3/5\n",
      "65421/65421 [==============================] - 8255s 126ms/step - loss: 319.8077 - rmse: 17.8827 - mae: 13.4080 - val_loss: 365.0760 - val_rmse: 19.1070 - val_mae: 14.4891\n",
      "Epoch 4/5\n",
      "65421/65421 [==============================] - 8509s 130ms/step - loss: 308.4506 - rmse: 17.5625 - mae: 13.0929 - val_loss: 362.7497 - val_rmse: 19.0460 - val_mae: 14.3507\n",
      "Epoch 5/5\n",
      "15093/65421 [=====>........................] - ETA: 1:51:34 - loss: 293.7235 - rmse: 17.1378 - mae: 12.6668"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=5, validation_data=val_ds, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history2 = model.fit(train_ds, epochs=5, validation_data=val_ds, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6la3cmZHCRD"
   },
   "outputs": [],
   "source": [
    "preds = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSIXHNNLINIM"
   },
   "outputs": [],
   "source": [
    "preds = preds_norm.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtiizrVWT1q7"
   },
   "outputs": [],
   "source": [
    "# def reconstruct(results, test, preds):\n",
    "#     test_df = test.copy() # make a copy of the data to conserve the original data\n",
    "#     if 'Yield' in test_df.columns:\n",
    "#         test_df.drop(columns=['Yield'], inplace=True)\n",
    "#     test_df['yield'] = preds\n",
    "#     # make a copy of the columns to be reused\n",
    "#     cols = test_df.columns\n",
    "#     operations = results['operations']\n",
    "#     n_operations = len(operations)\n",
    "#     for op in range(n_operations - 1, -1, -1):\n",
    "#         test_df = pd.merge(test_df, operations[op], on=operations[op].columns[0])\n",
    "#         test_df['yield'] = test_df['yield_mean'] + test_df['yield'] #* test_df['std_yied']\n",
    "#         test_df = test_df[cols]\n",
    "#     return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IGHer3xIOst"
   },
   "outputs": [],
   "source": [
    "# plt.hist(preds_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_test_df = reconstruct(results, test, preds_norm)\n",
    "# preds = preds_test_df['yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYsuUFNEIZMR"
   },
   "outputs": [],
   "source": [
    "plt.hist(test['Yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZYY1DaIJ95A"
   },
   "outputs": [],
   "source": [
    "plt.scatter(preds, test['Yield'])\n",
    "plt.plot([50, 300], [50, 300], 'r-')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8unLRzBWxfz"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(preds, test['Yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyWy414rzBAW"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train loss', 'val loss'])\n",
    "plt.xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['rmse'])\n",
    "plt.plot(history.history['val_rmse'])\n",
    "plt.legend(['train RMSE', 'val RMSE'])\n",
    "plt.xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test['Yield'] - preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('./savedModels/model_18_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_ds2 = df_to_dataset(train, batch_size=batch_size, feature_cols=features_cols, shuffle=False)\n",
    "# tr_preds = model.predict(tr_ds2)\n",
    "# tr_preds = tr_preds.squeeze()\n",
    "\n",
    "# plt.scatter(train['Yield'], tr_preds)\n",
    "# plt.plot([10, 300], [10, 300], 'r-')\n",
    "# plt.xlabel('actual')\n",
    "# plt.ylabel('predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.min(train['Yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_layers[0]._dense.kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_layers[0]._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[28].embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_features['year'] #['2016']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.groupby(['Location'])['Yield'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data.groupby(['trial_cluster_level1'])['Yield'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_grp = train.groupby(['Location'])['Yield'].agg(yield_mean='mean', std_yied='std').reset_index().sort_values('yield_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loc = pd.merge(train, loc_grp, on='Location')\n",
    "train_loc['Yield'] = (train_loc['Yield'] - train_loc['yield_mean']) /(train_loc['std_yied'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loc.groupby(['trial_cluster_level2'])['Yield'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_loc['Yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loc = train_loc[train.columns]\n",
    "cluster1_grp = train_loc.groupby(['trial_cluster_level1'])['Yield'].agg(yield_mean='mean', std_yied='std').reset_index().sort_values('yield_mean')\n",
    "train_cluster = pd.merge(train_loc, cluster1_grp, on='trial_cluster_level1')\n",
    "train_cluster['Yield'] = (train_cluster['Yield'] - train_cluster['yield_mean']) / (train_cluster['std_yied'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_cluster['Yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_cluster.groupby(['Year'])['Yield'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cluster = train_cluster[train.columns]\n",
    "year_grp = train_cluster.groupby(['Year'])['Yield'].agg(yield_mean='mean', std_yied='std').reset_index().sort_values('yield_mean')\n",
    "train_year = pd.merge(train_cluster, year_grp, on='Year')\n",
    "train_year['Yield'] = (train_year['Yield'] - train_year['yield_mean']) / (train_year['std_yied'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_year['Yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_year.groupby(['Year'])['Yield'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_year['Yield'].sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
